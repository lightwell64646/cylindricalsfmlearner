#!/usr/bin/env python
#
# gen_depth
# ---------
# Converts a directory of RGB images to a RGB/depth map video
#
# The purpose of this script is to generate depth visualizations from input
# images and videos. The command-line structure is at the top of the file,
# followed by the depth map prediction and the final output generation.
#
# For more help on how to use this script, run:
#
#   gen_depth --help
#
from __future__ import division
import argparse
import os
import glob
import tqdm
import sys
import numpy as np
from PIL import Image
import tensorflow as tf
from SfMLearner import SfMLearner
from utils import normalize_depth_for_display
import cv2
os.environ['TF_CPP_MIN_LOG_LEVEL']='2'


#################### PARSER #

parser = argparse.ArgumentParser(
    description='Convert a directory of RGB images to a depth map video.'
)

# positional args
parser.add_argument('target', help='directory of images to be converted')

# flags
parser.add_argument('-o', '--outfile', metavar='outfile',
                                       help='output file name (not including ext)',
                                       default='out',
                                       required=False)
parser.add_argument('--width', metavar='w',
                               type=int,
                               help='input image width in pixels',
                               default=512,
                               required=False)
parser.add_argument('--height', metavar='h',
                                type=int,
                                help='input image height in pixels',
                                default=128,
                                required=False)
parser.add_argument('--cmap', metavar='color_map', default='plasma', help='color map for depth')
parser.add_argument('-s', '--stack', metavar='type',
                                     help='options for stacking the target and depth map',
                                     choices=['horizontal','vertical','none'],
                                     default='vertical',
                                     required=False)
parser.add_argument('--checkpoint', metavar='c',
                                    help='tensorflow checkpoint file to use',
                                    default='checkpoints/model.latest',
                                    required=False)
args = parser.parse_args()

# config
target = args.target.strip()
img_height = args.height
img_width  = args.width
ckpt_file  = args.checkpoint
stack      = args.stack
outfile    = args.outfile


#################### PREDICT DEPTH #

# setup graph
print('Setting up TensorFlow graph...')
tf.reset_default_graph() # for reruns
sfm = SfMLearner()
sfm.setup_inference(img_height,
                    img_width,
                    mode='depth')

# preprocess video
targets = sorted(glob.glob(target + '/*.png'))

# start output video
width_out = img_width
height_out  = img_height
if stack == 'horizontal':
    width_out *= 2
elif stack == 'vertical':
    height_out *= 2
videowriter = cv2.VideoWriter(outfile + '.mp4',cv2.VideoWriter_fourcc('m','p','4','v'),30,(width_out,height_out))

# load model
saver = tf.train.Saver([var for var in tf.model_variables()])
with tf.Session() as sess:
    saver.restore(sess, ckpt_file)

    # open/resize image
    i=1
    m=len(targets)
    for i,target in enumerate(tqdm.tqdm(targets)):
        fh = open(target, 'rb')
        I = Image.open(fh)
        I = I.resize((img_width, img_height), Image.ANTIALIAS)
        I = np.array(I)

        # run model
        pred = sfm.inference(I[None,:,:,:], sess, mode='depth')

        # output depth image
        depth = normalize_depth_for_display(pred['depth'][0,:,:,0], cmap=args.cmap)
        O = np.uint8(depth*255)
        # stack output images as requested
        if stack == 'horizontal':
            out = np.append(I, O, 1)
        elif stack == 'vertical':
            out = np.append(I, O, 0)
        else:
            out = O
        videowriter.write(out[:,:,::-1])

